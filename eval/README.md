This folder provides scripts to reproduce evaluation results across various benchmarks for the **Qwen** series of large language models.

## Supported Benchmarks

Currently, we support the following benchmark:

| Model | Dataset | Config | Reproduced Score |
|-------|--------|--------|------------------|
| Qwen3-235B-A22B-Instruct-2507 | ARC-AGI 1 (pass@1) | [./configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml](./configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml) | 40.75 |

In the meantime, you can find the model outputs and final evaluation results in the [`./output`](./output) and [`./eval_res`](./eval_res) directories, respectively.

Additional benchmarks will be added in future updates. 


## Evaluation Guide

Follow the steps below to reproduce the reported scores.

### Step 0: Prerequisites

Ensure you have:
- Python â‰¥ 3.9
- Either [vLLM](https://github.com/vllm-project/vllm) or [SGLang](https://github.com/sgl-project/sgl) installed

Install required dependencies:

```bash
pip install -r requirements.txt
```

### Step 1: Start vLLM Server

Launch the vLLM inference server using the command below:

```bash
export MODEL_NAME="Qwen/Qwen3-235B-A22B-Instruct-2507"  # Replace with desired model
export MODEL_PATH="$MODEL_NAME"  # Or path to local checkpoint
export NUM_GPUS=8

python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL_PATH" \
    --trust-remote-code \
    --served-model-name "$MODEL_NAME" \
    --tensor-parallel-size $NUM_GPUS \
    --enforce-eager \
    --port 8030
```

> ğŸ’¡ Adjust `tensor_parallel_size` according to your GPU setup.

### Optional: Start SGLang Router (Recommended for Faster Evaluation)

Since evaluations can take several days, we recommend using **SGLang** with data parallelism to accelerate inference. See the [SGLang Router documentation](https://docs.sglang.ai/router/router.html) for details.

Start the SGLang router server:

```bash
python -m sglang_router.launch_server \
    --model-path Qwen/Qwen3-235B-A22B-Instruct-2507 \
    --dp-size 4 \
    --host 0.0.0.0 \
    --port 30000
```

> âš ï¸ Adjust `dp_size` based on available resources, and ensure consistency in port configuration for subsequent steps.


### Step 2: Run Inference

Once the inference server is running, generate model responses using the multithreaded inference script.

```bash
mkdir -p output

# Example: Evaluate on ARC-AGI
python generate_api_answers/infer_multithread.py \
    --config configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml
```

#### Resume Interrupted Inference

If the process is interrupted, simply re-run the same command. The script will automatically detect existing outputs and resume generation for incomplete prompts.

### Step 3: Compute Scores

After inference completes, evaluate the results using the scoring script:

```bash
mkdir -p eval_res

python eval/eval.py \
    --config configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml \
    > eval_res/ARCAGI-Qwen3-235B-A22B-Instruct-2507_eval_result.txt
```

The final score will be saved to the specified output file.

---

## ä¸­æ–‡ç¿»è¯‘

æœ¬æ–‡ä»¶å¤¹æä¾›è„šæœ¬ï¼Œç”¨äºå¤ç° **Qwen** ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç±»è¯„æµ‹åŸºå‡†ä¸Šçš„ç»“æœã€‚

## æ”¯æŒçš„è¯„æµ‹

ç›®å‰æ”¯æŒä»¥ä¸‹è¯„æµ‹ï¼š

| æ¨¡å‹ | æ•°æ®é›† | é…ç½® | å¤ç°åˆ†æ•° |
|------|--------|------|----------|
| Qwen3-235B-A22B-Instruct-2507 | ARC-AGI 1 (pass@1) | [./configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml](./configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml) | 40.75 |

åŒæ—¶ï¼Œä½ å¯ä»¥åœ¨ [`./output`](./output) ä¸ [`./eval_res`](./eval_res) ç›®å½•ä¸­åˆ†åˆ«æ‰¾åˆ°æ¨¡å‹è¾“å‡ºå’Œæœ€ç»ˆè¯„æµ‹ç»“æœã€‚

åç»­æ›´æ–°å°†ä¼šåŠ å…¥æ›´å¤šè¯„æµ‹ã€‚


## è¯„æµ‹æŒ‡å—

æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¤ç°æŠ¥å‘Šä¸­çš„åˆ†æ•°ã€‚

### æ­¥éª¤ 0ï¼šå‰ç½®æ¡ä»¶

è¯·ç¡®ä¿å…·å¤‡ï¼š
- Python â‰¥ 3.9
- å·²å®‰è£… [vLLM](https://github.com/vllm-project/vllm) æˆ– [SGLang](https://github.com/sgl-project/sgl) ä¹‹ä¸€

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

### æ­¥éª¤ 1ï¼šå¯åŠ¨ vLLM æœåŠ¡

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ vLLM æ¨ç†æœåŠ¡ï¼š

```bash
export MODEL_NAME="Qwen/Qwen3-235B-A22B-Instruct-2507"  # å¯æ›¿æ¢ä¸ºç›®æ ‡æ¨¡å‹
export MODEL_PATH="$MODEL_NAME"  # æˆ–æœ¬åœ°æƒé‡è·¯å¾„
export NUM_GPUS=8

python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL_PATH" \
    --trust-remote-code \
    --served-model-name "$MODEL_NAME" \
    --tensor-parallel-size $NUM_GPUS \
    --enforce-eager \
    --port 8030
```

> ğŸ’¡ æ ¹æ® GPU èµ„æºè°ƒæ•´ `tensor_parallel_size`ã€‚

### å¯é€‰ï¼šå¯åŠ¨ SGLang Routerï¼ˆæ¨èåŠ é€Ÿè¯„æµ‹ï¼‰

ç”±äºè¯„æµ‹å¯èƒ½æŒç»­æ•°å¤©ï¼Œå»ºè®®ä½¿ç”¨ **SGLang** çš„æ•°æ®å¹¶è¡Œæ¥åŠ é€Ÿæ¨ç†ã€‚è¯¦æƒ…è§ [SGLang Router æ–‡æ¡£](https://docs.sglang.ai/router/router.html)ã€‚

å¯åŠ¨ SGLang è·¯ç”±æœåŠ¡ï¼š

```bash
python -m sglang_router.launch_server \
    --model-path Qwen/Qwen3-235B-A22B-Instruct-2507 \
    --dp-size 4 \
    --host 0.0.0.0 \
    --port 30000
```

> âš ï¸ è¯·æ ¹æ®èµ„æºè°ƒæ•´ `dp_size`ï¼Œå¹¶ä¿è¯åç»­æ­¥éª¤çš„ç«¯å£é…ç½®ä¸€è‡´ã€‚


### æ­¥éª¤ 2ï¼šè¿è¡Œæ¨ç†

æ¨ç†æœåŠ¡å¯åŠ¨åï¼Œä½¿ç”¨å¤šçº¿ç¨‹æ¨ç†è„šæœ¬ç”Ÿæˆæ¨¡å‹è¾“å‡ºã€‚

```bash
mkdir -p output

# ç¤ºä¾‹ï¼šåœ¨ ARC-AGI ä¸Šè¯„æµ‹
python generate_api_answers/infer_multithread.py \
    --config configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml
```

#### æ–­ç‚¹ç»­è·‘

å¦‚æœä¸­é€”ä¸­æ–­ï¼Œç›´æ¥é‡å¤æ‰§è¡ŒåŒä¸€å‘½ä»¤å³å¯ã€‚è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å·²æœ‰è¾“å‡ºï¼Œå¹¶ç»§ç»­ç”Ÿæˆæœªå®Œæˆçš„éƒ¨åˆ†ã€‚

### æ­¥éª¤ 3ï¼šè®¡ç®—åˆ†æ•°

æ¨ç†å®Œæˆåï¼Œä½¿ç”¨è¯„åˆ†è„šæœ¬è®¡ç®—ç»“æœï¼š

```bash
mkdir -p eval_res

python eval/eval.py \
    --config configs/ARCAGI-Qwen3-235B-A22B-Instruct-2507.yaml \
    > eval_res/ARCAGI-Qwen3-235B-A22B-Instruct-2507_eval_result.txt
```

æœ€ç»ˆåˆ†æ•°å°†ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶ä¸­ã€‚

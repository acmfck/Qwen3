# Qwen3

<p align="center">
    <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"/>
<p>

<p align="center">
          ğŸ’œ <a href="https://chat.qwen.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2505.09388">Paper</a> &nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen3/">Blog</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ“– <a href="https://qwen.readthedocs.io/">Documentation</a>
<br>
ğŸ–¥ï¸ <a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo">Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp
</p>


è®¿é—®æˆ‘ä»¬çš„ Hugging Face æˆ– ModelScope ç»„ç»‡ï¼ˆç‚¹å‡»ä¸Šæ–¹é“¾æ¥ï¼‰ï¼Œæœç´¢ä»¥ `Qwen3-` å¼€å¤´çš„æƒé‡ï¼Œæˆ–è®¿é—® [Qwen3 collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)ï¼Œå³å¯æ‰¾åˆ°æ‰€éœ€å†…å®¹ï¼ç¥ä½¿ç”¨æ„‰å¿«ï¼

æƒ³äº†è§£æ›´å¤š Qwen3ï¼Œæ¬¢è¿é˜…è¯»æˆ‘ä»¬çš„æ–‡æ¡£ \[[EN](https://qwen.readthedocs.io/en/latest/)|[ZH](https://qwen.readthedocs.io/zh-cn/latest/)\]ã€‚æ–‡æ¡£åŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

- Quickstartï¼šåŸºç¡€ç”¨æ³•ä¸æ¼”ç¤ºï¼›
- Inferenceï¼šåŸºäº Transformers çš„æ¨ç†æŒ‡å—ï¼ŒåŒ…æ‹¬æ‰¹é‡æ¨ç†ã€æµå¼è¾“å‡ºç­‰ï¼›
- Run Locallyï¼šåœ¨ CPU/GPU æœ¬åœ°è¿è¡Œ LLM çš„è¯´æ˜ï¼Œæ¶µç›– llama.cppã€Ollamaã€LM Studio ç­‰æ¡†æ¶ï¼›
- Deploymentï¼šä½¿ç”¨ SGLangã€vLLMã€TGI ç­‰æ¡†æ¶è¿›è¡Œå¤§è§„æ¨¡æ¨ç†éƒ¨ç½²çš„ç¤ºä¾‹ï¼›
- Quantizationï¼šä½¿ç”¨ GPTQã€AWQ è¿›è¡Œé‡åŒ–ï¼Œä»¥åŠåˆ¶ä½œé«˜è´¨é‡ GGUF é‡åŒ–æ–‡ä»¶çš„æŒ‡å—ï¼›
- Trainingï¼šåè®­ç»ƒæŒ‡å—ï¼ŒåŒ…æ‹¬ä½¿ç”¨ Axolotlã€LLaMA-Factory ç­‰æ¡†æ¶è¿›è¡Œ SFT å’Œ RLHFï¼ˆTODOï¼‰ï¼›
- Frameworkï¼šåœ¨ RAGã€Agent ç­‰åº”ç”¨æ¡†æ¶ä¸­ä½¿ç”¨ Qwen çš„æ–¹æ³•ã€‚

## ç®€ä»‹

### Qwen3-2507

åœ¨è¿‡å»ä¸‰ä¸ªæœˆé‡Œï¼Œæˆ‘ä»¬æŒç»­æ¢ç´¢ Qwen3 ç³»åˆ—çš„æ½œåŠ›ï¼Œå¹¶å¾ˆé«˜å…´æ¨å‡ºæ›´æ–°ç‰ˆæœ¬ **Qwen3-2507**ã€‚è¯¥ç‰ˆæœ¬åŒ…å« Qwen3-Instruct-2507 å’Œ Qwen3-Thinking-2507 ä¸¤ä¸ªå˜ä½“ï¼Œå¹¶æä¾› 235B-A22Bã€30B-A3Bã€4B ä¸‰ç§è§„æ¨¡ã€‚

**Qwen3-Instruct-2507** æ˜¯æ­¤å‰ Qwen3 éæ€è€ƒæ¨¡å¼çš„æ›´æ–°ç‰ˆæœ¬ï¼Œå¸¦æ¥å¦‚ä¸‹å…³é”®æå‡ï¼š  

- **é€šç”¨èƒ½åŠ›æ˜¾è‘—æå‡**ï¼Œè¦†ç›– **æŒ‡ä»¤è·Ÿéšã€é€»è¾‘æ¨ç†ã€æ–‡æœ¬ç†è§£ã€æ•°å­¦ã€ç§‘å­¦ã€ä»£ç ä¸å·¥å…·ä½¿ç”¨**ã€‚  
- **å¤šè¯­è¨€é•¿å°¾çŸ¥è¯†è¦†ç›–æ˜¾è‘—å¢å¼º**ã€‚  
- **åœ¨ä¸»è§‚ä¸å¼€æ”¾å¼ä»»åŠ¡ä¸Šæ›´å¥½å¯¹é½ç”¨æˆ·åå¥½**ï¼Œç”Ÿæˆæ›´æœ‰å¸®åŠ©ä¸”æ›´é«˜è´¨é‡çš„å›å¤ã€‚  
- **256K é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¢å¼º**ï¼Œå¯æ‰©å±•è‡³ **100 ä¸‡ tokens**ã€‚

**Qwen3-Thinking-2507** æ˜¯ Qwen3 æ€è€ƒæ¨¡å‹çš„å»¶ç»­ï¼Œåœ¨æ¨ç†è´¨é‡ä¸æ·±åº¦ä¸Šè¿›ä¸€æ­¥æå‡ï¼Œå…³é”®æ”¹è¿›åŒ…æ‹¬ï¼š
- **æ¨ç†ä»»åŠ¡è¡¨ç°æ˜¾è‘—æå‡**ï¼Œè¦†ç›–é€»è¾‘æ¨ç†ã€æ•°å­¦ã€ç§‘å­¦ã€ä»£ç ä¸å­¦æœ¯åŸºå‡†ï¼ˆé€šå¸¸éœ€è¦äººç±»ä¸“å®¶å‚ä¸ï¼‰ï¼Œè¾¾åˆ° **å¼€æºæƒé‡æ€è€ƒæ¨¡å‹çš„ SOTA æ°´å¹³**ã€‚
- **é€šç”¨èƒ½åŠ›æ˜¾è‘—å¢å¼º**ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è·Ÿéšã€å·¥å…·ä½¿ç”¨ã€æ–‡æœ¬ç”Ÿæˆä¸äººç±»åå¥½å¯¹é½ã€‚
- **256K é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¢å¼º**ï¼Œå¯æ‰©å±•è‡³ **100 ä¸‡ tokens**ã€‚


<details>
    <summary><b>æ­¤å‰çš„ Qwen3 ç‰ˆæœ¬</b></summary>
    <h3>Qwen3ï¼ˆäº¦ç§° Qwen3-2504ï¼‰</h3>
    <p>
    æˆ‘ä»¬å¾ˆé«˜å…´å‘å¸ƒ Qwen3ï¼Œè¿™æ˜¯ Qwen å¤§è¯­è¨€æ¨¡å‹å®¶æ—çš„æœ€æ–°æˆå‘˜ã€‚
    è¿™äº›æ¨¡å‹åŸºäºæˆ‘ä»¬åœ¨ QwQ ä¸ Qwen2.5 æ–¹é¢çš„ç§¯ç´¯ï¼Œæ˜¯è¿„ä»Šæœ€å…ˆè¿›ã€æœ€æ™ºèƒ½çš„ç³»ç»Ÿã€‚
    æˆ‘ä»¬å·²å°† Qwen3 æƒé‡å¼€æ”¾ï¼ŒåŒ…æ‹¬ç¨ å¯†æ¨¡å‹ä¸æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ã€‚
    <br><br>
    Qwen3 çš„äº®ç‚¹åŒ…æ‹¬ï¼š
        <ul>
            <li><b>å¤šç§è§„æ¨¡çš„ç¨ å¯†ä¸ MoE æ¨¡å‹</b>ï¼Œæä¾› 0.6Bã€1.7Bã€4Bã€8Bã€14Bã€32Bã€30B-A3B ä¸ 235B-A22Bã€‚</li>
            <li><b>æ€è€ƒæ¨¡å¼</b>ï¼ˆç”¨äºå¤æ‚é€»è¾‘æ¨ç†ã€æ•°å­¦ä¸ç¼–ç ï¼‰ä¸<b>éæ€è€ƒæ¨¡å¼</b>ï¼ˆé«˜æ•ˆé€šç”¨å¯¹è¯ï¼‰ä¹‹é—´å¯æ— ç¼åˆ‡æ¢ï¼Œç¡®ä¿ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä½³è¡¨ç°ã€‚</li>
            <li><b>æ¨ç†èƒ½åŠ›æ˜¾è‘—å¢å¼º</b>ï¼Œåœ¨æ•°å­¦ã€ä»£ç ç”Ÿæˆä¸å¸¸è¯†é€»è¾‘æ¨ç†ç­‰æ–¹é¢è¶…è¿‡æ­¤å‰ QwQï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ä¸ Qwen2.5 Instructï¼ˆéæ€è€ƒæ¨¡å¼ï¼‰ã€‚</li>
            <li><b>æ›´å¥½çš„äººç±»åå¥½å¯¹é½</b>ï¼Œåœ¨åˆ›æ„å†™ä½œã€è§’è‰²æ‰®æ¼”ã€å¤šè½®å¯¹è¯ä¸æŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¸¦æ¥æ›´è‡ªç„¶ã€æ›´æœ‰æ²‰æµ¸æ„Ÿçš„å¯¹è¯ä½“éªŒã€‚</li>
            <li><b>å…·å¤‡å¼ºå¤§ Agent èƒ½åŠ›</b>ï¼Œå¯åœ¨æ€è€ƒä¸éæ€è€ƒæ¨¡å¼ä¸‹ç²¾å‡†è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œåœ¨å¤æ‚ Agent ä»»åŠ¡ä¸Šè¾¾åˆ°å¼€æºæ¨¡å‹é¢†å…ˆæ°´å¹³ã€‚</li>
            <li><b>æ”¯æŒ 100+ è¯­è¨€ä¸æ–¹è¨€</b>ï¼Œå…·å¤‡å‡ºè‰²çš„<b>å¤šè¯­è¨€æŒ‡ä»¤è·Ÿéš</b>ä¸<b>ç¿»è¯‘</b>èƒ½åŠ›ã€‚</li>
        </ul>
    </p>
</details>


## æ–°é—»
- 2025.08.08ï¼šQwen3-2507 å·²æ”¯æŒ **100 ä¸‡ tokens** çš„è¶…é•¿è¾“å…¥ï¼è¯·æŸ¥çœ‹æ›´æ–°çš„æ¨¡å‹å¡ï¼ˆ[235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)ã€[235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)ã€[A30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)ã€[A30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)ï¼‰äº†è§£å¦‚ä½•å¯ç”¨è¯¥ç‰¹æ€§ã€‚
- 2025.08.06ï¼šQwen3-2507 æœ€ç»ˆå¼€æ”¾ç‰ˆæœ¬ [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) ä¸ [Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507) å‘å¸ƒï¼
- 2025.07.31ï¼šQwen3-30B-A3B-Thinking-2507 å‘å¸ƒã€‚è¯¦æƒ…è§ [modelcard](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)ã€‚
- 2025.07.30ï¼šQwen3-30B-A3B-Instruct-2507 å‘å¸ƒã€‚è¯¦æƒ…è§ [modelcard](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)ã€‚
- 2025.07.25ï¼šQwen3-235B-A22B æ€è€ƒæ¨¡å¼æ›´æ–°ç‰ˆæœ¬ Qwen3-235B-A22B-Thinking-2507 å‘å¸ƒã€‚è¯¦æƒ…è§ [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)ã€‚
- 2025.07.21ï¼šQwen3-235B-A22B éæ€è€ƒæ¨¡å¼æ›´æ–°ç‰ˆæœ¬ Qwen3-235B-A22B-Instruct-2507 å‘å¸ƒï¼Œå¸¦æ¥æ˜¾è‘—å¢å¼ºå¹¶æ”¯æŒ 256K é•¿ä¸Šä¸‹æ–‡ç†è§£ã€‚è¯¦æƒ…è§ [modelcard](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)ã€‚
- 2025.04.29ï¼šQwen3 ç³»åˆ—å‘å¸ƒã€‚è¯¦æƒ…è§ [blog](https://qwenlm.github.io/blog/qwen3)ã€‚
- 2024.09.19ï¼šQwen2.5 ç³»åˆ—å‘å¸ƒï¼Œå¹¶æ–°å¢ 3Bã€14Bã€32B ä¸‰ç§è§„æ¨¡ã€‚è¯¦æƒ…è§ [blog](https://qwenlm.github.io/blog/qwen2.5)ã€‚
- 2024.06.06ï¼šQwen2 ç³»åˆ—å‘å¸ƒã€‚è¯¦æƒ…è§ [blog](https://qwenlm.github.io/blog/qwen2/)ï¼
- 2024.03.28ï¼šå‘å¸ƒé¦–ä¸ª Qwen MoE æ¨¡å‹ï¼šQwen1.5-MoE-A2.7Bï¼ç›®å‰ä»… HF transformers ä¸ vLLM æ”¯æŒè¯¥æ¨¡å‹ï¼Œåç»­å°†æ”¯æŒ llama.cppã€mlx-lm ç­‰ã€‚è¯¦æƒ…è§ [blog](https://qwenlm.github.io/blog/qwen-moe/)ã€‚
- 2024.02.05ï¼šQwen1.5 ç³»åˆ—å‘å¸ƒã€‚

## æ€§èƒ½

è¯¦ç»†è¯„æµ‹ç»“æœè§ [ğŸ“‘ blog (Qwen3-2504)](https://qwenlm.github.io/blog/qwen3/) ä¸ [ğŸ“‘ blog (Qwen3-2507) \[å³å°†å‘å¸ƒ\]]()ã€‚

GPU æ˜¾å­˜éœ€æ±‚ä¸ååé‡ç»“æœå¯å‚è€ƒ[æ­¤å¤„](https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html)ã€‚

## è¿è¡Œ Qwen3

### ğŸ¤— Transformers

Transformers æ˜¯ç”¨äºæ¨ç†ä¸è®­ç»ƒçš„é¢„è®­ç»ƒ NLP åº“ã€‚
æ¨èä½¿ç”¨æœ€æ–°ç‰ˆï¼Œå¹¶è¦æ±‚ `transformers>=4.51.0`ã€‚

#### Qwen3-Instruct-2507

ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Qwen3-30B-A3B-Instruct-2507 æ ¹æ®è¾“å…¥ç”Ÿæˆå†…å®¹ã€‚
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Instruct-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)
```

> [!Note]
> Qwen3-Instruct-2507 ä»…æ”¯æŒéæ€è€ƒæ¨¡å¼ï¼Œè¾“å‡ºä¸­ä¸ä¼šåŒ…å« ``<think></think>`` å—ã€‚åŒæ—¶ä¸å†éœ€è¦æ˜¾å¼è®¾ç½® `enable_thinking=False`ã€‚


#### Qwen3-Thinking-2507

ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Qwen3-30B-A3B-Thinking-2507 æ ¹æ®è¾“å…¥ç”Ÿæˆå†…å®¹ã€‚
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Thinking-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)  # no opening <think> tag
print("content:", content)

```

> [!Note]
> Qwen3-Thinking-2507 ä»…æ”¯æŒæ€è€ƒæ¨¡å¼ã€‚
> å¦å¤–ï¼Œä¸ºå¼ºåˆ¶æ¨¡å‹æ€è€ƒï¼Œé»˜è®¤èŠå¤©æ¨¡æ¿ä¼šè‡ªåŠ¨åŠ å…¥ `<think>`ã€‚å› æ­¤æ¨¡å‹è¾“å‡ºä¸­å¯èƒ½åªå‡ºç° `</think>` è€Œæ²¡æœ‰æ˜¾å¼çš„ `<think>` èµ·å§‹æ ‡ç­¾ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚
> 
> Qwen3-Thinking-2507 è¿˜æ”¯æŒæ›´é•¿çš„æ€è€ƒé•¿åº¦ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ä¸ºå…¶è®¾ç½®è¶³å¤Ÿçš„æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‚



<details>
    <summary><b>æ­¤å‰ Qwen3 æ¨¡å‹çš„æ€è€ƒ/éæ€è€ƒæ¨¡å¼åˆ‡æ¢</b></summary>
    <p>
    é»˜è®¤æƒ…å†µä¸‹ï¼ŒQwen3 æ¨¡å‹ä¼šåœ¨å›å¤å‰è¿›è¡Œæ€è€ƒã€‚
    å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼æ§åˆ¶ï¼š
        <ul>
            <li><code>enable_thinking=False</code>ï¼šåœ¨ `tokenizer.apply_chat_template` ä¸­ä¼ å…¥ <code>enable_thinking=False</code> å¯ä¸¥æ ¼ç¦æ­¢æ¨¡å‹ç”Ÿæˆæ€è€ƒå†…å®¹ã€‚</li>
            <li><code>/think</code> ä¸ <code>/no_think</code> æŒ‡ä»¤ï¼šåœ¨ system æˆ– user æ¶ˆæ¯ä¸­ä½¿ç”¨è¿™äº›è¯æ¥æŒ‡ç¤º Qwen3 æ˜¯å¦æ€è€ƒã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œéµå¾ªæœ€æ–°æŒ‡ä»¤ã€‚</li>
        </ul>
    </p>
</details>


### ModelScope

æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼ˆå°¤å…¶æ˜¯ä¸­å›½å¤§é™†ç”¨æˆ·ï¼‰ä½¿ç”¨ ModelScopeã€‚
ModelScope æä¾›ä¸ Transformers ç±»ä¼¼çš„ Python APIã€‚
`modelscope download` CLI å·¥å…·å¯å¸®åŠ©è§£å†³æƒé‡ä¸‹è½½é—®é¢˜ã€‚
å¯¹äº vLLM ä¸ SGLangï¼Œå¯åˆ†åˆ«è®¾ç½®ç¯å¢ƒå˜é‡ `VLLM_USE_MODELSCOPE=true` ä¸ `SGLANG_USE_MODELSCOPE=true`ã€‚


### llama.cpp

[`llama.cpp`](https://github.com/ggml-org/llama.cpp) å¯åœ¨å¹¿æ³›ç¡¬ä»¶ä¸Šä»¥æå°‘é…ç½®å®ç°é«˜æ€§èƒ½ LLM æ¨ç†ã€‚
å»ºè®®ä½¿ç”¨ `llama.cpp>=b5401` ä»¥å®Œæ•´æ”¯æŒ Qwen3ã€‚

åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ CLIï¼š
```shell
./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift
# CTRL+C to exit
```

åœ¨ç»ˆç«¯ä¸­å¯åŠ¨ API serverï¼š
```shell
./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080
```
ç®€å•çš„ Web å‰ç«¯ä½äº `http://localhost:8080`ï¼ŒOpenAI å…¼å®¹ API ä½äº `http://localhost:8080/v1`ã€‚

æ›´å¤šæŒ‡å—è¯·å‚è€ƒ[æ–‡æ¡£](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html)ã€‚

> [!Note]
> llama.cpp é‡‡ç”¨â€œæ—‹è½¬ä¸Šä¸‹æ–‡ç®¡ç†â€ï¼Œé€šè¿‡é€å‡ºæ—©æœŸ token æ¥å®ç°æ— é™ç”Ÿæˆã€‚
> å¯é€šè¿‡å‚æ•°é…ç½®ï¼Œä¸Šè¿°å‘½ä»¤å®é™…ä¸Šç¦ç”¨äº†è¿™ä¸€æœºåˆ¶ã€‚
> è¯¦æƒ…è¯·å‚è€ƒ[æ–‡æ¡£](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli)ã€‚

### Ollama

åœ¨[å®‰è£… Ollama](https://ollama.com/) åï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨æœåŠ¡ï¼ˆæ¨è Ollama v0.9.0 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰ï¼š
```shell
ollama serve
# You need to keep this service running whenever you are using ollama
```

ä½¿ç”¨ `ollama run` æ‹‰å–å¹¶è¿è¡Œæ¨¡å‹ã€‚å¯é€šè¿‡ `qwen3` çš„åç¼€æŒ‡å®šæ¨¡å‹è§„æ¨¡ï¼Œä¾‹å¦‚ `:8b` æˆ– `:30b-a3b`ï¼š
```shell
ollama run qwen3:8b
# Setting parameters, type "/set parameter num_ctx 40960" and "/set parameter num_predict 32768"
# To exit, type "/bye" and press ENTER
# For Qwen3-2504 models,
# - To enable thinking, which is the default, type "/set think"
# - To disable thinking, type "/set nothink"
```

ä¹Ÿå¯é€šè¿‡ OpenAI å…¼å®¹ API ä½¿ç”¨ Ollamaã€‚
è¯·æ³¨æ„ï¼š(1) ä½¿ç”¨ API æ—¶éœ€ä¿æŒ `ollama serve` è¿è¡Œï¼›(2) åœ¨è°ƒç”¨ API å‰éœ€æ‰§è¡Œ `ollama run qwen3:8b`ï¼Œä»¥ç¡®ä¿æ¨¡å‹æƒé‡å°±ç»ªã€‚
é»˜è®¤ API åœ°å€ä¸º `http://localhost:11434/v1/`ã€‚

æ›´å¤šä¿¡æ¯è¯·è®¿é—® [ollama.ai](https://ollama.com/)ã€‚

> [!Note]
> Ollama çš„å‘½åå¯èƒ½ä¸ Qwen çš„åŸå§‹å‘½åä¸å®Œå…¨ä¸€è‡´ã€‚
> ä¾‹å¦‚ï¼Œæˆªè‡³ 2025 å¹´ 8 æœˆï¼ŒOllama ä¸­çš„ `qwen3:30b-a3b` æŒ‡å‘ `qwen3:30b-a3b-thinking-2507-q4_K_M`ã€‚
> ä½¿ç”¨å‰è¯·æŸ¥çœ‹ <https://ollama.com/library/qwen3/tags>ã€‚


> [!Note]
> Ollama ä¸ llama.cpp ä¸€æ ·é‡‡ç”¨â€œæ—‹è½¬ä¸Šä¸‹æ–‡ç®¡ç†â€ã€‚
> ä½†å…¶é»˜è®¤è®¾ç½®ï¼ˆ`num_ctx` 2048ã€`num_predict` -1ï¼‰æ„å‘³ç€åœ¨ 2048 token ä¸Šä¸‹æ–‡å†…çš„æ— é™ç”Ÿæˆï¼Œ
> å¯èƒ½å¯¼è‡´ Qwen3 æ¨¡å‹å‡ºç°é—®é¢˜ã€‚
> å»ºè®®åˆç†è®¾ç½® `num_ctx` ä¸ `num_predict`ã€‚

### LMStudio

Qwen3 å·²è¢« [lmstudio.ai](https://lmstudio.ai/) æ”¯æŒï¼Œå¯ç›´æ¥ä½¿ç”¨æˆ‘ä»¬çš„ GGUF æ–‡ä»¶ã€‚

### ExecuTorch

å¦‚éœ€å¯¼å‡ºå¹¶åœ¨ ExecuTorchï¼ˆiOSã€Androidã€Macã€Linux ç­‰ï¼‰ä¸Šè¿è¡Œï¼Œè¯·å‚è€ƒæ­¤[ç¤ºä¾‹](https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md)ã€‚

### MNN

å¦‚éœ€å¯¼å‡ºå¹¶åœ¨ç§»åŠ¨ç«¯æ”¯æŒ Qwen3 çš„ MNN ä¸Šè¿è¡Œï¼Œè¯·è®¿é—® [Alibaba MNN](https://github.com/alibaba/MNN)ã€‚

### MLX LM

åœ¨ Apple Silicon ä¸Šè¿è¡Œæ—¶ï¼Œ[`mlx-lm`](https://github.com/ml-explore/mlx-lm) ä¹Ÿæ”¯æŒ Qwen3ï¼ˆ`mlx-lm>=0.24.0`ï¼‰ã€‚
å¯åœ¨ Hugging Face Hub ä¸­å¯»æ‰¾ä»¥ MLX ç»“å°¾çš„æ¨¡å‹ã€‚


### OpenVINO

åœ¨ Intel CPU æˆ– GPU ä¸Šè¿è¡Œæ—¶ï¼Œ[OpenVINO toolkit](https://github.com/openvinotoolkit) æ”¯æŒ Qwen3ã€‚
å¯å‚è€ƒæ­¤[èŠå¤©æœºå™¨äººç¤ºä¾‹](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llm-chatbot/llm-chatbot.ipynb)ã€‚


## éƒ¨ç½² Qwen3

Qwen3 æ”¯æŒå¤šç§æ¨ç†æ¡†æ¶ã€‚
è¿™é‡Œæ¼”ç¤º `SGLang`ã€`vLLM` ä¸ `TensorRT-LLM` çš„ç”¨æ³•ã€‚
ä½ ä¹Ÿå¯ä»¥åœ¨ä¸åŒæ¨ç†æœåŠ¡å•†å¤„è·å– Qwen3 æ¨¡å‹ï¼Œä¾‹å¦‚ [Alibaba Cloud Model Studio](https://www.alibabacloud.com/en/product/modelstudio)ã€‚


### SGLang

[SGLang](https://github.com/sgl-project/sglang) æ˜¯ç”¨äºå¤§è¯­è¨€æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜é€Ÿæ¨ç†æ¡†æ¶ã€‚
SGLang å¯å¯åŠ¨ OpenAI å…¼å®¹ API æœåŠ¡ã€‚
éœ€è¦ `sglang>=0.4.6.post1`ã€‚

å¯¹äº Qwen3-Instruct-2507ï¼š
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --port 30000 --context-length 262144
```

å¯¹äº Qwen3-Thinking-2507ï¼š
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --port 30000 --context-length 262144 --reasoning-parser deepseek-r1
```

å¯¹äº Qwen3ï¼š
```shell
python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --context-length 131072 --reasoning-parser qwen3
```
OpenAI å…¼å®¹ API åœ°å€ä¸º `http://localhost:30000/v1`ã€‚

> [!Note]
> ç”±äº SGLang é¢„å¤„ç† API è¯·æ±‚æ—¶ä¼šç§»é™¤æ‰€æœ‰ `reasoning_content` å­—æ®µï¼Œå¯¼è‡´ **Qwen3 æ€è€ƒæ¨¡å‹åœ¨å¤šæ­¥å·¥å…·è°ƒç”¨** ä¸­çš„æ•ˆæœå¯èƒ½ä¸ç†æƒ³ï¼ˆè¿™ç±»ä»»åŠ¡éœ€è¦ç›¸å…³æ€è€ƒå†…å®¹ï¼‰ã€‚ä¿®å¤æ­£åœ¨è¿›è¡Œä¸­ã€‚
> ä½œä¸ºä¸´æ—¶æ–¹æ¡ˆï¼Œå»ºè®®ç›´æ¥ä¼ å…¥åŸå§‹å†…å®¹ï¼Œä¸è¦æŠ½å–æ€è€ƒå†…å®¹ï¼ŒèŠå¤©æ¨¡æ¿å°†æ­£ç¡®å¤„ç†ã€‚


### vLLM

[vLLM](https://github.com/vllm-project/vllm) æ˜¯é«˜ååã€å†…å­˜é«˜æ•ˆçš„ LLM æ¨ç†ä¸æœåŠ¡å¼•æ“ã€‚
æ¨èä½¿ç”¨ `vllm>=0.9.0`ã€‚

å¯¹äº Qwen3-Instruct-2507ï¼š
```shell
vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --port 8000 --max-model-len 262144
```

å¯¹äº Qwen3-Thinking-2507ï¼š
```shell
vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --port 8000 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1
```

å¯¹äº Qwen3ï¼š
```shell
vllm serve Qwen/Qwen3-8B --port 8000 --max-model-len 131072 --enable-reasoning --reasoning-parser qwen3
```
OpenAI å…¼å®¹ API åœ°å€ä¸º `http://localhost:8000/v1`ã€‚

> [!Note]
> ç”±äº vLLM é¢„å¤„ç† API è¯·æ±‚æ—¶ä¼šç§»é™¤æ‰€æœ‰ `reasoning_content` å­—æ®µï¼Œå¯¼è‡´ **Qwen3 æ€è€ƒæ¨¡å‹åœ¨å¤šæ­¥å·¥å…·è°ƒç”¨** ä¸­çš„æ•ˆæœå¯èƒ½ä¸ç†æƒ³ï¼ˆè¿™ç±»ä»»åŠ¡éœ€è¦ç›¸å…³æ€è€ƒå†…å®¹ï¼‰ã€‚ä¿®å¤æ­£åœ¨è¿›è¡Œä¸­ã€‚
> ä½œä¸ºä¸´æ—¶æ–¹æ¡ˆï¼Œå»ºè®®ç›´æ¥ä¼ å…¥åŸå§‹å†…å®¹ï¼Œä¸è¦æŠ½å–æ€è€ƒå†…å®¹ï¼ŒèŠå¤©æ¨¡æ¿å°†æ­£ç¡®å¤„ç†ã€‚

### TensorRT-LLM

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) æ˜¯ NVIDIA å¼€æºçš„ LLM æ¨ç†å¼•æ“ï¼Œæä¾›è‡ªå®šä¹‰ attention kernelã€é‡åŒ–ç­‰ä¼˜åŒ–ã€‚
Qwen3 å·²åœ¨å…¶é‡æ„åçš„ [PyTorch backend](https://nvidia.github.io/TensorRT-LLM/torch.html) ä¸­å¾—åˆ°æ”¯æŒã€‚
æ¨è `tensorrt_llm>=0.20.0rc3`ã€‚
æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [README](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/core/qwen/README.md#qwen3)ã€‚

```shell
trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch
```
OpenAI å…¼å®¹ API åœ°å€ä¸º `http://localhost:8000/v1`ã€‚

### MindIE

åœ¨ Ascend NPU ä¸Šéƒ¨ç½²è¯·è®¿é—® [Modelers](https://modelers.cn/) å¹¶æœç´¢ Qwen3ã€‚

<!-- 
### OpenLLM

[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily runÂ Qwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:

```bash
openllm serve qwen2.5:7b
```

The server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). -->


## åŸºäº Qwen3 æ„å»º

### Tool Use

é’ˆå¯¹å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå»ºè®®æŸ¥çœ‹ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ï¼Œå®ƒæä¾›äº†è¿™äº› API çš„å°è£…ï¼Œå¹¶æ”¯æŒ MCP çš„å·¥å…·è°ƒç”¨æˆ–å‡½æ•°è°ƒç”¨ã€‚
Qwen3 çš„å·¥å…·è°ƒç”¨ä¹Ÿå¯é€šè¿‡ SGLangã€vLLMã€Transformersã€llama.cppã€Ollama ç­‰å®ç°ã€‚
å…·ä½“å¯ç”¨æ–¹å¼è¯·å‚è€ƒæ–‡æ¡£ã€‚


### Finetuning

å»ºè®®ä½¿ç”¨è®­ç»ƒæ¡†æ¶ï¼ˆå¦‚ [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)ã€[UnSloth](https://github.com/unslothai/unsloth)ã€[Swift](https://github.com/modelscope/swift)ã€[Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) ç­‰ï¼‰è¿›è¡Œ SFTã€DPOã€GRPO ç­‰å¾®è°ƒã€‚


## è®¸å¯åè®®

æˆ‘ä»¬çš„å¼€æºæƒé‡æ¨¡å‹å‡é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ã€‚
è®¸å¯è¯æ–‡ä»¶å¯åœ¨å¯¹åº”çš„ Hugging Face ä»“åº“ä¸­æ‰¾åˆ°ã€‚

## å¼•ç”¨

å¦‚æœæˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚

```bibtex
@article{qwen3,
    title={Qwen3 Technical Report}, 
    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
    journal = {arXiv preprint arXiv:2505.09388},
    year={2025}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@article{qwen2,
    title   = {Qwen2 Technical Report}, 
    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
    journal = {arXiv preprint arXiv:2407.10671},
    year    = {2024}
}
```

## è”ç³»æˆ‘ä»¬
å¦‚éœ€è”ç³»ç ”ç©¶å›¢é˜Ÿæˆ–äº§å“å›¢é˜Ÿï¼Œè¯·åŠ å…¥æˆ‘ä»¬çš„ [Discord](https://discord.gg/z3GAxXZ9Ce) æˆ– [å¾®ä¿¡ç¾¤](assets/wechat.png)ï¼
